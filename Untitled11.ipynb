{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "authorship_tag": "ABX9TyO/cH+3ko/NN3DNF4WYumVh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yling01/15799-project1/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAu9Au-Lxdcg",
        "outputId": "1b194b6d-cde0-4a9b-be98-bdf3e73f4057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as ttf\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "import math\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "!pip install timm\n",
        "from timm.models.layers import DropPath"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "\n",
        "VERSION = '12'\n",
        "PLATFORM = 'Collab'\n",
        "CONTINUE = False \n",
        "\n",
        "if PLATFORM == 'AWS':\n",
        "    PATH = '/home/ubuntu/efs/11785/hw2p2/Models'\n",
        "else:\n",
        "    PATH = '/content/drive/MyDrive/CMU/11785/hw2p2/Models/'\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "PATH = PATH + VERSION + '/'\n",
        "try:\n",
        "    os.makedirs(PATH)\n",
        "except OSError as error:\n",
        "    print(error)   \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwdzP9DcxnUn",
        "outputId": "ab51f3cf-13fd-48a3-de49-1a3680d963b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 17] File exists: '/content/drive/MyDrive/CMU/11785/hw2p2/Models/12/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "# !mkdir /root/.kaggle\n",
        "\n",
        "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "#     f.write('{\"username\":\"yingjieling\",\"key\":\"5a27d5620222819ea5c3b71d2251e19c\"}') # Put your kaggle username & key here\n",
        "\n",
        "# !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# !kaggle competitions download -c 11-785-s22-hw2p2-classification\n",
        "# !kaggle competitions download -c 11-785-s22-hw2p2-verification\n",
        "\n",
        "# !unzip -q 11-785-s22-hw2p2-classification.zip\n",
        "# !unzip -q 11-785-s22-hw2p2-verification.zip"
      ],
      "metadata": {
        "id": "f4yUAt9Yxx9j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --no-cache-dir --upgrade comet_ml\n",
        "from comet_ml import Experiment\n",
        "experiment = Experiment(\n",
        "    api_key=\"fs5JWzC05BHp2mS1s7w1OUTz5\",\n",
        "    project_name=\"11785-hw2p2\",\n",
        "    workspace=\"yling01\",\n",
        ")\n",
        "\n",
        "experiment.set_name(\"{} {}\".format(VERSION, PLATFORM))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GpxJOULx1Ew",
        "outputId": "c0c543ff-c423-4d4a-c05c-9f3f58f32c7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.7/dist-packages (3.28.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.23.0)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (3.0.2)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (2.9.0)\n",
            "Requirement already satisfied: dulwich>=0.20.6 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (0.20.33)\n",
            "Requirement already satisfied: everett[ini]>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (0.9.1)\n",
            "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (1.13.3)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (4.3.3)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.6->comet_ml) (2021.10.8)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.7/dist-packages (from dulwich>=0.20.6->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.7/dist-packages (from everett[ini]>=1.0.1->comet_ml) (5.0.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (3.10.0.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (5.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema!=3.1.0,>=2.6.0->comet_ml) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18.4->comet_ml) (2.10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: sklearn, torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET ERROR: Error logging git-related information\n",
            "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/yling01/11785-hw2p2/a5030335cc2947468d7b47104deed48e\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The well-accepted SGD batch_size & lr combination for CNN classification is 256 batch size for 0.1 learning rate.\n",
        "When changing batch size for SGD, follow the linear scaling rule - halving batch size -> halve learning rate, etc.\n",
        "This is less theoretically supported for Adam, but in my experience, it's a decent ballpark estimate.\n",
        "\"\"\"\n",
        "batch_size = 64\n",
        "lr = 0.01\n",
        "epochs = 100 # Just for the early submission. We'd want you to train like 50 epochs for your main submissions."
      ],
      "metadata": {
        "id": "vLs360Qxx21Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block_ConvNext(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 stride,\n",
        "                 expand_ratio):\n",
        "        \n",
        "        super().__init__() \n",
        "\n",
        "        if stride == 1 and in_channels == out_channels:\n",
        "            self.do_identity = True\n",
        "        else:\n",
        "            self.do_identity = False\n",
        "\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "\n",
        "        #depth wise\n",
        "        self.spatial_mixing = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=7, padding=3, stride=stride, groups=in_channels, bias=False),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "        )\n",
        "\n",
        "        #point wise \n",
        "        self.feature_mixing = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, padding=0, stride=stride, bias=True),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        #point wise \n",
        "        self.bottleneck_channels = nn.Sequential(\n",
        "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, padding=0, stride=stride, bias=True),\n",
        "        )\n",
        "\n",
        "        self.drop_path = DropPath(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.spatial_mixing(x)\n",
        "        out = self.feature_mixing(out)\n",
        "        out = self.bottleneck_channels(out)\n",
        "\n",
        "        if self.do_identity:\n",
        "            return x + self.drop_path(out)\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "\n",
        "class ConvNext(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=7000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=4, stride=4, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(96),\n",
        "        )\n",
        "\n",
        "        self.stage_cfgs = [\n",
        "            # expand_ratio, channels, # blocks, stride of first block\n",
        "            [4, 96, 3, 1],\n",
        "            [4, 192, 3, 1],\n",
        "            [4, 384, 9, 1],\n",
        "            [4, 768, 3, 1],\n",
        "        ]\n",
        "\n",
        "        in_channels = 96\n",
        "        downsample_dims = [192, 384, 768]\n",
        "        layers = []\n",
        "        for index, curr_stage in enumerate(self.stage_cfgs):\n",
        "            expand_ratio, num_channels, num_blocks, stride = curr_stage\n",
        "\n",
        "            for block_idx in range(num_blocks):\n",
        "                out_channels = num_channels\n",
        "                layers.append(Block_ConvNext(\n",
        "                    in_channels=in_channels,\n",
        "                    out_channels=out_channels,\n",
        "                    stride=stride,\n",
        "                    expand_ratio=expand_ratio\n",
        "                ))\n",
        "                # In channels of the next block is the out_channels of the current one\n",
        "                in_channels = out_channels\n",
        "            if index < 3:\n",
        "                layers.append(\n",
        "                    nn.Sequential(\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.Conv2d(out_channels, downsample_dims[index], kernel_size=2, stride=2),\n",
        "                    )\n",
        "                )\n",
        "                in_channels = downsample_dims[index]\n",
        "\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)  # Done, save them to the class\n",
        "\n",
        "        # Now, we need to build the final classification layer.\n",
        "        self.cls_layer = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),  # Pool over & collapse the spatial dimensions to (1, 1)\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.Flatten(),  # Collapse the trivial (1, 1) dimensions\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_channels, num_classes)  # Project to our # of classes\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "        out = self.stem(x)\n",
        "        feats = self.layers(out)\n",
        "        out = self.cls_layer(feats)\n",
        "        if return_feats:\n",
        "            return feats\n",
        "        else:\n",
        "            return out"
      ],
      "metadata": {
        "id": "BnJeEYjsx4jd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Transforms (data augmentation) is quite important for this task.\n",
        "Go explore https://pytorch.org/vision/stable/transforms.html for more details\n",
        "\"\"\"\n",
        "torch.manual_seed(0)\n",
        "DATA_DIR = \"\"\n",
        "TRAIN_DIR = osp.join(DATA_DIR, \"classification/classification/train\") # This is a smaller subset of the data. Should change this to classification/classification/train\n",
        "VAL_DIR = osp.join(DATA_DIR, \"classification/classification/dev\")\n",
        "TEST_DIR = osp.join(DATA_DIR, \"classification/classification/test\")\n",
        "\n",
        "train_transforms = [ttf.RandomHorizontalFlip(), \n",
        "                    ttf.RandomResizedCrop((224, 224), scale=(0.25, 1)),\n",
        "                    ttf.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
        "                    ttf.RandAugment(),\n",
        "                    ttf.ToTensor()]\n",
        "val_transforms = [ttf.ToTensor()]\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR,\n",
        "                                                 transform=ttf.Compose(train_transforms))\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR,\n",
        "                                               transform=ttf.Compose(val_transforms))\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          shuffle=True, drop_last=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        drop_last=True, num_workers=1)"
      ],
      "metadata": {
        "id": "YBAqBzl6x6ZV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNext()\n",
        "model.cuda()\n",
        "\n",
        "# For this homework, we're limiting you to 35 million trainable parameters, as\n",
        "# outputted by this. This is to help constrain your search space and maintain\n",
        "# reasonable training times & expectations\n",
        "num_trainable_parameters = 0\n",
        "for p in model.parameters():\n",
        "    num_trainable_parameters += p.numel()\n",
        "print(\"Number of Params: {}\".format(num_trainable_parameters))\n",
        "\n",
        "# TODO: What criterion do we use for this task?\n",
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n",
        "\n",
        "# For this homework, we strongly strongly recommend using FP16 to speed up training.\n",
        "# It helps more for larger models.\n",
        "# Go to https://effectivemachinelearning.com/PyTorch/8._Faster_training_with_mixed_precision\n",
        "# and compare \"Single precision training\" section with \"Mixed precision training\" section\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCQoA4pHx8Jh",
        "outputId": "24ec97cc-1dca-4535-a2c8-1d132f101184"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Params: 33189784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_epoch = 0\n",
        "if CONTINUE:\n",
        "    models = os.listdir(PATH)\n",
        "    models.sort(key=lambda x: int(x.split(\"_\")[-1]))\n",
        "    model_path = PATH + models[-1]\n",
        "    true_epoch = int(model_path.split(\"_\")[-1])\n",
        "    print(\"!!!!!!Warning!!!!!!\\n continuing from \\n\\t{}\\n\\n\".format(model_path))\n",
        "    temp = torch.load(model_path)\n",
        "    model.load_state_dict(temp['model_state_dict'])\n",
        "    optimizer.load_state_dict(temp['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(temp['scheduler_state_dict'])\n",
        "\n",
        "for epoch in range(current_epoch + 1, epochs + 1):\n",
        "        \n",
        "    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        # Don't be surprised - we just wrap these two lines to make it work for FP16\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "\n",
        "        # Update # correct & loss as we go\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "        scheduler.step()\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    # training metrics\n",
        "    train_acc = 100 * num_correct / (len(train_loader) * batch_size)\n",
        "    train_loss = float(total_loss / len(train_loader))\n",
        "\n",
        "    # validation metrics\n",
        "    model.eval()\n",
        "    num_correct = 0\n",
        "    for i, (x, y) in enumerate(val_loader):\n",
        "        x = x.cuda()\n",
        "        y = y.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n",
        "    validation_acc = 100 * num_correct / len(val_dataset)\n",
        "    \n",
        "\n",
        "    # print results\n",
        "    print(\"Epoch {}/{}: Train Acc {:.04f}%, Validation Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
        "        epoch,\n",
        "        epochs,\n",
        "        train_acc,\n",
        "        validation_acc,\n",
        "        train_loss,\n",
        "        float(optimizer.param_groups[0]['lr'])))\n",
        "    \n",
        "    # metric export\n",
        "    experiment.log_metric(\"train acc\", train_acc, epoch=epoch)\n",
        "    experiment.log_metric(\"validation acc\", validation_acc, epoch=epoch)\n",
        "\n",
        "    # model saving\n",
        "    torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict' : scheduler.state_dict(),\n",
        "        }, PATH+\"Model_\"+str(epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "O6SoagnIx-Je",
        "outputId": "51e60849-b696-4c0b-d2f2-41f8d626d794"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100: Train Acc 0.0193%, Validation Acc 0.0286%, Train Loss 8.9338, Learning Rate 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/100: Train Acc 0.0307%, Validation Acc 0.1114%, Train Loss 8.7281, Learning Rate 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/100: Train Acc 0.2079%, Validation Acc 0.5629%, Train Loss 8.2585, Learning Rate 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/100: Train Acc 1.1653%, Validation Acc 2.2857%, Train Loss 7.5508, Learning Rate 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/100: Train Acc 3.6951%, Validation Acc 7.6600%, Train Loss 6.8648, Learning Rate 0.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/100: Train Acc 8.9742%, Validation Acc 15.9914%, Train Loss 6.1839, Learning Rate 0.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/100: Train Acc 18.0327%, Validation Acc 26.1200%, Train Loss 5.4735, Learning Rate 0.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/100: Train Acc 28.5865%, Validation Acc 35.1114%, Train Loss 4.8210, Learning Rate 0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/100: Train Acc 38.8932%, Validation Acc 42.9657%, Train Loss 4.2626, Learning Rate 0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100: Train Acc 47.9660%, Validation Acc 50.8714%, Train Loss 3.8061, Learning Rate 0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/100: Train Acc 55.7192%, Validation Acc 55.0314%, Train Loss 3.4377, Learning Rate 0.0097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/100: Train Acc 61.8191%, Validation Acc 58.4314%, Train Loss 3.1453, Learning Rate 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/100: Train Acc 67.3733%, Validation Acc 60.4400%, Train Loss 2.8922, Learning Rate 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/100: Train Acc 71.9393%, Validation Acc 61.1286%, Train Loss 2.6879, Learning Rate 0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/100: Train Acc 75.7223%, Validation Acc 63.4543%, Train Loss 2.5210, Learning Rate 0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/100: Train Acc 79.3088%, Validation Acc 64.3857%, Train Loss 2.3654, Learning Rate 0.0094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/100: Train Acc 82.2852%, Validation Acc 65.5686%, Train Loss 2.2427, Learning Rate 0.0093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/100: Train Acc 85.0573%, Validation Acc 65.9143%, Train Loss 2.1306, Learning Rate 0.0092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/100: Train Acc 86.9349%, Validation Acc 66.7543%, Train Loss 2.0524, Learning Rate 0.0091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/100: Train Acc 88.8032%, Validation Acc 67.8171%, Train Loss 1.9732, Learning Rate 0.0090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/100: Train Acc 89.8062%, Validation Acc 68.6114%, Train Loss 1.9287, Learning Rate 0.0090\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/100: Train Acc 91.2587%, Validation Acc 69.1743%, Train Loss 1.8674, Learning Rate 0.0089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/100: Train Acc 91.8796%, Validation Acc 68.8400%, Train Loss 1.8371, Learning Rate 0.0088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/100: Train Acc 92.5326%, Validation Acc 70.4914%, Train Loss 1.8041, Learning Rate 0.0086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/100: Train Acc 93.1249%, Validation Acc 70.5057%, Train Loss 1.7781, Learning Rate 0.0085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   5%|▌         | 119/2187 [00:27<07:58,  4.32it/s, acc=93.8542%, loss=1.7392, lr=0.0085, num_correct=7208]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ce0a7ac6c5e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# Another couple things you need for FP16.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is something added just for FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_init_scale_growth_tracker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# Invoke the more complex machinery only if we're treating multiple outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "wQC3bZ3_yALe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eneg_g2z44oh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}